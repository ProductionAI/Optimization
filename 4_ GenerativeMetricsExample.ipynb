{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5ec7baa",
   "metadata": {},
   "source": [
    "Let's break down the types of LLM metrics, their pros and cons, and how to interpret them.  LLM evaluation is a complex and evolving field, so this is a general overview.\n",
    "\n",
    "# I. Categories of LLM Metrics\n",
    "\n",
    "LLM metrics can be broadly categorized into:\n",
    "\n",
    "Text Quality Metrics: These assess the quality of the generated text itself.\n",
    "\n",
    "Task-Specific Metrics: These evaluate how well the LLM performs on a particular task.\n",
    "\n",
    "Efficiency Metrics: These measure the computational resources required by the LLM.\n",
    "\n",
    "Safety and Bias Metrics: These assess the potential for harmful or biased outputs.\n",
    "\n",
    "# II. Text Quality Metrics\n",
    "\n",
    "## Perplexity\n",
    "Measures how well a language model predicts the next word in a sequence.  Lower perplexity is generally better.\n",
    "\n",
    "Advantages: Easy to calculate, widely used.\n",
    "Disadvantages: Doesn't directly correlate with human judgment of quality, sensitive to vocabulary and text domain.\n",
    "Interpretation: A perplexity of 20 means the model is, on average, 20 times less certain about the next word than it would be if it guessed perfectly. Comparing perplexity scores between models trained on the same data can be useful.\n",
    "## BLEU (Bilingual Evaluation Understudy)\n",
    "Measures the overlap of n-grams between the generated text and one or more reference texts.\n",
    "\n",
    "Advantages: Easy to calculate, widely used.\n",
    "Disadvantages: Doesn't capture semantic similarity, sensitive to word order variations, can be less reliable for highly creative text.\n",
    "Interpretation: BLEU scores range from 0 to 1 (or 0 to 100). Higher scores indicate better overlap with the reference text. A BLEU score of 0.5 means that, according to the metric, 50% of the n-grams in the generated text are also present in the reference text.\n",
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation):  Similar to BLEU, but focuses on recall (how much of the reference text is captured by the generated text).  Variations include ROUGE-L (longest common subsequence), ROUGE-1 (unigram overlap), ROUGE-2 (bigram overlap).\n",
    "\n",
    "Advantages: Considers different aspects of overlap than BLEU, various ROUGE scores offer different perspectives.\n",
    "Disadvantages: Still relies on n-gram matching, doesn't capture semantic similarity.\n",
    "Interpretation: ROUGE scores also range from 0 to 1. Higher scores are better. ROUGE-L measures how well the generated text captures the longest common subsequence of the reference text.\n",
    "## METEOR (Metric for Evaluation of Translation)\n",
    "Considers synonyms and paraphrases, attempting to capture semantic similarity.\n",
    "\n",
    "Advantages: Better at capturing meaning than BLEU and ROUGE.\n",
    "Disadvantages: More complex to calculate.\n",
    "Interpretation: METEOR scores range from 0 to 1. Higher is better.\n",
    "## BERTScore\n",
    "Uses contextualized word embeddings from BERT to measure similarity between generated and reference texts at a semantic level.\n",
    "\n",
    "Advantages: Captures semantic similarity well.\n",
    "Disadvantages: Computationally more expensive.\n",
    "Interpretation: BERTScore outputs a similarity score between 0 and 1. Higher is better.\n",
    "## Human Evaluation\n",
    "The gold standard.  Humans judge the quality of the generated text based on criteria like fluency, coherence, relevance, and creativity.\n",
    "\n",
    "Advantages: Most accurate measure of quality.\n",
    "Disadvantages: Expensive, time-consuming, subjective, and difficult to scale.\n",
    "Interpretation: Human evaluation often involves assigning scores on Likert scales (e.g., from 1 to 5) or ranking different outputs.\n",
    "# III. Task-Specific Metrics\n",
    "\n",
    "These metrics depend on the specific task the LLM is designed for.  Examples:\n",
    "\n",
    "Accuracy, Precision, Recall, F1-score: Common metrics for classification tasks.\n",
    "Exact Match: Used for question answering.\n",
    "Mean Average Precision (MAP): Used for information retrieval.\n",
    "Reward Models: Used in Reinforcement Learning from Human Feedback (RLHF) to align LLM outputs with human preferences.\n",
    "IV. Efficiency Metrics\n",
    "\n",
    "Inference Speed: How quickly the LLM generates text.\n",
    "Memory Usage: The amount of memory required by the LLM.\n",
    "Computational Cost: The amount of computing power needed to run the LLM.\n",
    "V. Safety and Bias Metrics\n",
    "\n",
    "Toxicity: Measures the presence of toxic or offensive language in the generated text.\n",
    "Bias: Assesses whether the LLM exhibits biases towards certain groups of people. This is a complex and nuanced area. There are many types of bias.\n",
    "Adversarial Attacks: Measures the robustness of the LLM to malicious inputs designed to elicit harmful or biased outputs.\n",
    "How to Interpret Values\n",
    "\n",
    "Comparison is Key: Most metrics are most useful for comparing different LLMs or different versions of the same LLM.\n",
    "Context Matters: The interpretation of a metric depends on the specific task and data. A BLEU score of 0.5 might be good for one task but not for another.\n",
    "No Single Perfect Metric: LLM evaluation is multi-faceted. It's important to consider multiple metrics to get a comprehensive view of performance.\n",
    "Human Evaluation is Crucial: While automated metrics are useful, human evaluation is essential for assessing the overall quality and usefulness of LLM outputs.\n",
    "Important Note: The field of LLM evaluation is constantly evolving. New metrics and techniques are being developed all the time.  It's important to stay up-to-date with the latest research in this area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "537867fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Perplexity: 14.812183380126953\n",
      "Initial BLEU Score: 0.5444460596606694\n",
      "Initial Context Precision: 0.3333333333333333\n",
      "\n",
      "Iteration 1:\n",
      "Generated Text: The cat sat on a mat.\n",
      "Perplexity: 25.568115234375\n",
      "BLEU Score: 0.36409302398068727\n",
      "Context Precision: 0.3333333333333333\n",
      "\n",
      "Iteration 2:\n",
      "Generated Text: A cat sat on the mat.\n",
      "Perplexity: 22.17603302001953\n",
      "BLEU Score: 0.36409302398068727\n",
      "Context Precision: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# 1. Perplexity\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, text):\n",
    "    \"\"\"Calculates the perplexity of a given text using a language model.\"\"\"\n",
    "    encodings = tokenizer(text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(encodings['input_ids'], labels=encodings['input_ids'])\n",
    "    loss = outputs.loss\n",
    "    perplexity = torch.exp(loss)\n",
    "    return perplexity.item()\n",
    "\n",
    "\n",
    "# 2. BLEU Score\n",
    "\n",
    "def calculate_bleu(reference_text, generated_text):\n",
    "    \"\"\"Calculates the BLEU score between a reference and generated text.\"\"\"\n",
    "    reference_tokens = reference_text.split()  # Simple tokenization\n",
    "    generated_tokens = generated_text.split()\n",
    "    # Smoothing function to handle cases with few or no matching n-grams\n",
    "    smoothing = SmoothingFunction().method4  \n",
    "    score = sentence_bleu([reference_tokens], generated_tokens, smoothing_function=smoothing)\n",
    "    return score\n",
    "\n",
    "\n",
    "# 3. Context Precision (Simplified Example)\n",
    "\n",
    "def calculate_context_precision(context, generated_text, relevant_keywords):\n",
    "    \"\"\"Calculates context precision based on keyword overlap.\"\"\"\n",
    "    context_keywords = set(context.split())  # Simplified keyword extraction\n",
    "    generated_keywords = set(generated_text.split()) # Simplified keyword extraction\n",
    "\n",
    "    relevant_keywords_set = set(relevant_keywords)\n",
    "\n",
    "    # Intersection of generated keywords with relevant keywords within the context\n",
    "    relevant_generated_keywords = generated_keywords.intersection(relevant_keywords_set).intersection(context_keywords)\n",
    "\n",
    "    if len(generated_keywords) == 0:  # Avoid division by zero\n",
    "        return 0.0\n",
    "    \n",
    "    precision = len(relevant_generated_keywords) / len(generated_keywords)\n",
    "    return precision\n",
    "\n",
    "\n",
    "\n",
    "# Example Usage and Improvement Loop\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"gpt2\"  # Or any other suitable model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Example Data (Replace with your actual data)\n",
    "context = \"The cat sat on the mat.\"\n",
    "reference_text = \"The cat sat on the mat and purred.\"\n",
    "relevant_keywords = [\"cat\", \"mat\", \"sat\"]  # Keywords relevant to the context\n",
    "\n",
    "\n",
    "# Initial Evaluation\n",
    "generated_text = \"The cat sat on the mat.\"  # Example generated text\n",
    "perplexity = calculate_perplexity(model, tokenizer, context + \" \" + generated_text)\n",
    "bleu_score = calculate_bleu(reference_text, generated_text)\n",
    "context_precision = calculate_context_precision(context, generated_text, relevant_keywords)\n",
    "\n",
    "print(f\"Initial Perplexity: {perplexity}\")\n",
    "print(f\"Initial BLEU Score: {bleu_score}\")\n",
    "print(f\"Initial Context Precision: {context_precision}\")\n",
    "\n",
    "\n",
    "\n",
    "# Improvement Loop (Simplified - In real scenarios, you'd fine-tune)\n",
    "for i in range(2): # Example iterations\n",
    "    # 1.  (Simplified) Text Generation Change:  Introduce slight variation\n",
    "    if i == 0:\n",
    "        generated_text = \"The cat sat on a mat.\"  # Slight change\n",
    "    elif i == 1:\n",
    "      generated_text = \"A cat sat on the mat.\" # Another change\n",
    "\n",
    "    # 2. Re-evaluate\n",
    "    perplexity = calculate_perplexity(model, tokenizer, context + \" \" + generated_text)\n",
    "    bleu_score = calculate_bleu(reference_text, generated_text)\n",
    "    context_precision = calculate_context_precision(context, generated_text, relevant_keywords)\n",
    "\n",
    "    print(f\"\\nIteration {i+1}:\")\n",
    "    print(f\"Generated Text: {generated_text}\")\n",
    "    print(f\"Perplexity: {perplexity}\")\n",
    "    print(f\"BLEU Score: {bleu_score}\")\n",
    "    print(f\"Context Precision: {context_precision}\")\n",
    "\n",
    "\n",
    "    # 3. (In a real scenario) Fine-tuning:  You would use the metrics to guide fine-tuning \n",
    "    #    to improve the model's performance.  This is a simplified example.\n",
    "    #    You might adjust model parameters, training data, etc. based on the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc8e315",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
