{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb1b1df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size (original): 3099920\n",
      "Model Size (quantized): 1546240\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.quantization\n",
    "\n",
    "# 1. Define a smaller, simpler Transformer block\n",
    "class SmallTransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, intermediate_size):  # Added intermediate_size\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(hidden_size, num_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(hidden_size, intermediate_size),  # Smaller intermediate layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(intermediate_size, hidden_size)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = x + attn_output\n",
    "        x = self.norm1(x)\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + ff_output\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "# 2. Create the smaller LLM\n",
    "class SmallLLM(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, num_heads, intermediate_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.layers = nn.ModuleList([\n",
    "            SmallTransformerBlock(hidden_size, num_heads, intermediate_size)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.lm_head = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.lm_head(x)\n",
    "        return x\n",
    "\n",
    "# 3. Instantiate the model with smaller parameters\n",
    "vocab_size = 10000  # Example vocabulary size\n",
    "hidden_size = 128   # Reduced hidden size\n",
    "num_layers = 4      # Fewer layers\n",
    "num_heads = 4       # Fewer attention heads\n",
    "intermediate_size = 256 # Smaller intermediate size in feed-forward layer\n",
    "\n",
    "model = SmallLLM(vocab_size, hidden_size, num_layers, num_heads, intermediate_size)\n",
    "\n",
    "# 4. Quantization (Post-Training Dynamic Quantization)\n",
    "model.eval()  # Important: Set to evaluation mode before quantization\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model,  # the original model\n",
    "    {nn.Linear},  # a set of quantizable modules\n",
    "    dtype=torch.qint8  # the target dtype for quantized weights\n",
    ")\n",
    "\n",
    "# Example Usage (Inference):\n",
    "input_ids = torch.randint(0, vocab_size, (1, 64)) # Batch size 1, sequence length 64\n",
    "with torch.no_grad(): # Disable gradients for inference\n",
    "    outputs = quantized_model(input_ids)\n",
    "    # Process outputs...\n",
    "\n",
    "\n",
    "# --- Optional: Pruning (Illustrative) ---\n",
    "# (Requires a training loop to determine importances)\n",
    "# import torch.nn.utils.prune as prune\n",
    "\n",
    "# # Example: Prune 20% of weights in the first linear layer of the first block\n",
    "# module = model.layers[0].feed_forward[0]\n",
    "# prune.l1_unstructured(module, name=\"weight\", amount=0.2)\n",
    "# prune.remove(module, 'weight') # Permanently remove pruned weights\n",
    "\n",
    "# --- Optional: Export to ONNX or other formats for deployment ---\n",
    "# torch.onnx.export(quantized_model, (input_ids,), \"small_llm.onnx\")\n",
    "\n",
    "\n",
    "print(\"Model Size (original):\", sum(p.numel() for p in model.parameters()))\n",
    "print(\"Model Size (quantized):\", sum(p.numel() for p in quantized_model.parameters()))\n",
    "\n",
    "large_model = model\n",
    "small_model = quantized_model\n",
    "\n",
    "\n",
    "# You'll likely see a reduction in the number of parameters after quantization.\n",
    "# The actual memory footprint reduction will be more significant due to int8 storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6a17442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer  # Or your preferred tokenizer\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",  # Pad to max_length\n",
    "            truncation=True,        # Truncate if longer than max_length\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"     # Return PyTorch tensors\n",
    "        )\n",
    "        input_ids = encoding[\"input_ids\"].squeeze() # Remove batch dimension\n",
    "        labels = input_ids.clone()  # Labels are the same as input_ids for LM\n",
    "        # Mask padding tokens in labels. -100 is commonly used for this.\n",
    "        labels[encoding[\"attention_mask\"].squeeze() == 0] = -100\n",
    "        return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "\n",
    "\n",
    "def create_test_dataloader(texts, tokenizer_name, max_length, batch_size):\n",
    "    \"\"\"Creates a DataLoader for the test set.\"\"\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    dataset = TextDataset(texts, tokenizer, max_length)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False) # No need to shuffle test data\n",
    "    return dataloader, tokenizer\n",
    "\n",
    "\n",
    "# Example Usage:\n",
    "texts = [\n",
    "    \"This is the first test sentence.\",\n",
    "    \"Another example sentence for testing.\",\n",
    "    \"This is a longer sentence to demonstrate truncation.\",  # Example of long sentence\n",
    "    # ... more test sentences ...\n",
    "]\n",
    "\n",
    "tokenizer_name = \"bert-base-uncased\"  # Or your model's tokenizer\n",
    "max_length = 128  # Adjust as needed\n",
    "batch_size = 32 # Adjust based on your memory and needs\n",
    "\n",
    "test_dataloader, tokenizer = create_test_dataloader(texts, tokenizer_name, max_length, batch_size)\n",
    "\n",
    "\n",
    "# Now you can use test_dataloader in your evaluation loop:\n",
    "# for batch in test_dataloader:\n",
    "#     input_ids = batch['input_ids']\n",
    "#     labels = batch['labels']\n",
    "#     # ... (Rest of your evaluation code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "933a4da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Use GPU\n",
    "    print(f\"Using device: {device}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Use CPU\n",
    "    print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "106f4d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 02:49:17.816622: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9373] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-20 02:49:17.816650: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-20 02:49:17.817627: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1534] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-20 02:49:17.822997: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m perplexity\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# Return as a Python number\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Calculate and print the perplexity\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m perplexity \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerplexity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mperplexity\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 40\u001b[0m, in \u001b[0;36mcalculate_perplexity\u001b[0;34m(model, dataloader, device)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# No need to calculate gradients\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m---> 40\u001b[0m         input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     41\u001b[0m         labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     43\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(input_ids, labels\u001b[38;5;241m=\u001b[39mlabels)  \u001b[38;5;66;03m# Get model outputs\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM  # Or your preferred model\n",
    "\n",
    "# 1. Prepare your data\n",
    "#class TextDataset(Dataset):\n",
    "    #... (same as before)\n",
    "\n",
    "# Example usage:\n",
    "texts = [\n",
    "    \"This is the first test sentence.\",\n",
    "    \"Another example sentence for testing.\",\n",
    "    \"This is a longer sentence to demonstrate truncation.\",\n",
    "    #... more test sentences...\n",
    "]\n",
    "tokenizer_name = \"gpt2\"  # Example tokenizer\n",
    "max_length = 128       # Adjust as needed\n",
    "batch_size = 16        # Adjust as needed\n",
    "test_dataloader = create_test_dataloader(texts, tokenizer_name, max_length, batch_size)\n",
    "\n",
    "\n",
    "# 2. Load your language model\n",
    "model_name = \"gpt2\"  # Or your fine-tuned model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "\n",
    "# 3. Calculate perplexity\n",
    "def calculate_perplexity(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_words = 0\n",
    "    loss_fct = CrossEntropyLoss(ignore_index=-100)  # -100 is for padding tokens\n",
    "\n",
    "    with torch.no_grad():  # No need to calculate gradients\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, labels=labels)  # Get model outputs\n",
    "            loss = outputs.loss  # Directly get the loss from the model's output\n",
    "\n",
    "            # If your model doesn't directly return the loss:\n",
    "            # logits = outputs.logits\n",
    "            # loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "\n",
    "            total_loss += loss.item() * input_ids.size(0)  # Accumulate loss\n",
    "            total_words += torch.sum(labels!= -100).item()  # Count non-padding tokens\n",
    "\n",
    "    avg_loss = total_loss / total_words\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))  # Calculate perplexity\n",
    "    return perplexity.item()  # Return as a Python number\n",
    "\n",
    "# Calculate and print the perplexity\n",
    "perplexity = calculate_perplexity(model, test_dataloader, device)\n",
    "print(f\"Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b76e639",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
